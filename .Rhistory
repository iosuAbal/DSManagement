facet_wrap(~Metodo) +  # Utiliza facet_wrap para dividir los gráficos
theme_minimal()
g
# Crear el gráfico combinado con facet_wrap
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
facet_wrap(~Metodo)   # Utiliza facet_wrap para dividir los gráficos
g
# Crear el gráfico combinado con facet_wrap
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5) +
geom_point(data = df_combined[102, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[102, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5) +
facet_wrap(~Metodo)   # Utiliza facet_wrap para dividir los gráficos
g
# Crear el gráfico combinado con facet_wrap
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5) +
geom_point(data = df_combined[57+51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[57+51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5) +
facet_wrap(~Metodo)   # Utiliza facet_wrap para dividir los gráficos
g
# Crear el gráfico combinado con facet_wrap
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
geom_point(data = df_combined[57+51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[57+51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
facet_wrap(~Metodo)   # Utiliza facet_wrap para dividir los gráficos
g
View(data)
############################
# B: distancia correlación #
############################
c = cor(t(data), t(data))
d = sqrt(1-c)
d = as.dist(d)
d
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos que pueden haber 2 o 3 clusters
# Metodo particional para comprobarlo
calcular_silhouettes_pam(d,15) # El mayor silhouette se obtiene con k=2
#comparemos las particiones obtenidas en ambos casos
k <- 2
?pam
pam <- pam(d,k)
particiones <- comparar_particiones(k, hc, pam)
# En este caso hay 7 individuos clasificados de diferente manera
particionWard <- particiones[[1]]
particionPam <- particiones[[2]]
rbind(particionWard[particionPam==1]) # todos los ward y los que PAM sean 1
#teniendo en cuenta la particion que nos ha hecho el metodo pam
#interpretamos los datos: Calculamos la media de cada variable en cada cluster
interpretar_clusters(particionPam, data)
############################
# Reducción de las dimensiones
#aplicamos cmdscale
out <- calcular_scree_graph(d, 3)
############################
# Reducción de las dimensiones
#aplicamos cmdscale
out <- calcular_scree_graph(d, 2)
# La mayor parte de la información está representada con el primer valor
# Información que se retiene en cada caso
q = 1
sum(out$eig[1:q]) / sum(out$eig) # 0.98
q = 3
q = 2
sum(out$eig[1:q]) / sum(out$eig) # 0.998
# Creamos los dataframe para el grafico
df1 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionPam), Metodo = "PAM")
df2 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionWard), Metodo = "Ward")
df_combined <- rbind(df1, df2)
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
geom_point(data = df_combined[57+51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[57+51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
facet_wrap(~Metodo)
g
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[2, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[2, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
geom_point(data = df_combined[57+2, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[57+2, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
facet_wrap(~Metodo)
g
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
facet_wrap(~Metodo)
g
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam)
silWard = silhouette(particionWard, d)
plot(silWard)
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.61 de media, hay algunos individuos mal clasificados
silWard = silhouette(particionWard, d)
plot(silWard)#0.61 tambien, pero el cluster 2 es muy malo
g
#########################
# A: Distancia euclidea #
#########################
d = dist(data)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos k=2 clusters claros
# Metodo particional para comprobarlo
calcular_silhouettes_pam(d,15) # El mayor silhouette se obtiene con k=2
#comparemos las particiones obtenidas en ambos casos
k <- 2
pam <- pam(d,2)
particiones <- comparar_particiones(k, hc, pam)
# Vemos que son casi iguales
# Hay un solo elemento que no sale en el mismo cluster
# veamos cual es
particionWard <- particiones[[1]]
particionPam <- particiones[[2]]
rbind(particionWard[particionPam==1]) # todos los ward y los que PAM sean 1
#teniendo en cuenta la particion que nos ha hecho el metodo pam
#interpretamos los datos: Calculamos la media de cada variable en cada cluster
interpretar_clusters(particionPam, data)
############################
# Reducción de las dimensiones
#aplicamos cmdscale
out <- calcular_scree_graph(d, 2)
# La mayor parte de la información está representada con el primer valor
# Información que se retiene en cada caso
q = 1
sum(out$eig[1:q]) / sum(out$eig) # 0.98
q = 2
sum(out$eig[1:q]) / sum(out$eig) # 0.998
# Creamos los dataframe para el grafico
df1 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionPam), Metodo = "PAM")
df2 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionWard), Metodo = "Ward")
df_combined <- rbind(df1, df2)
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
geom_point(data = df_combined[57+51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[57+51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
facet_wrap(~Metodo)
g
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.61 de media, hay algunos individuos mal clasificados
silWard = silhouette(particionWard, d)
plot(silWard)#0.61 tambien, pero el cluster 2 es muy malo
View(data)
library(ggplot2)
#setwd("C:/Users/Alex/Desktop/Entrega3")
setwd("C:/UPV/Master/Analisis de datos/Entrega/Entrega3")
data = read.csv("indicedelitos.csv", header=TRUE, sep=" ")
#########################
# FUNCIONES COMUNES #
#########################
?hclust
calcular_dendrograma <- function(d, method="ward.D2"){
hc <- hclust(d, method= method)
plot(hc, hang=-0.1)
return (hc)
}
calcular_silhouettes <- function (d, n=10){
silPam = rep(NA, n)
for(k in 2:n)
{
#pam
p = pam(d, k, nstart=10)$clustering # en PAM hay que pasar la distancia
aux = silhouette(p, d) # aux es una matriz, lo que nosotros buscamos está en la tercera columna
silPam[k] = mean(aux[, 3])#sil_width
}
plot(1:n, silPam, type="b",xlab="Número de clusters", ylab="Silhouette width",ylim = c(0,1))
}
comparar_particiones <- function(k, hcObject, pamObject){
clward = cutree(hcObject, k)
clpam = pamObject$clustering
print(table(clward, clpam))
return(list(clward,clpam))
}
interpretar_clusters <- function(clpam, data){
for(cluster in 1:2)
{
selec = clpam==cluster
aux = apply(data[selec,], 2, function(x){c(mean(x), sd(x))})
cat("\nCluster ", cluster, "\n")
print(aux)
}
}
calcular_scree_graph <- function(d, k_p){
# Utilizando las coordenadas principales...
out = cmdscale(as.dist(d), k=k_p, eig=T, x.ret=T)
# Veamos los valores propios
plot(out$eig, type="b", xlim=c(0,10))
return(out)
}
#########################
# A: Distancia euclidea #
#########################
d = dist(data)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos k=2 clusters claros
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # El mayor silhouette se obtiene con k=2
pam <- pam(d,2)
# Vemos que son casi iguales
# Hay un solo elemento que no sale en el mismo cluster
# veamos cual es
particionWard <- particiones[[1]]
particionPam <- particiones[[2]]
rbind(particionWard[particionPam==1]) # todos los ward y los que PAM sean 1
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.66 de media
silWard = silhouette(particionWard, d)
plot(silWard)#0.65
#teniendo en cuenta la particion que nos ha hecho el metodo pam
#interpretamos los datos: Calculamos la media de cada variable en cada cluster
interpretar_clusters(particionPam, data)
#aplicamos cmdscale
out <- calcular_scree_graph(d, 2)
# La mayor parte de la información está representada con el primer valor
# Información que se retiene en cada caso
q = 1
sum(out$eig[1:q]) / sum(out$eig) # 0.98
q = 2
sum(out$eig[1:q]) / sum(out$eig) # 0.998
# Creamos los dataframe para el grafico
df1 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionPam), Metodo = "PAM")
df2 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionWard), Metodo = "Ward")
df_combined <- rbind(df1, df2)
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
geom_point(data = df_combined[57+51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[57+51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
facet_wrap(~Metodo)
g
############################
# B: distancia correlación #
############################
c = cor(t(data), t(data)) #ponemos individuos en columnas
d = sqrt(1-c)
d = as.dist(d)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos que pueden haber 2 o 3 clusters
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # Lo mejor es k=10, pero no es viable
#########################
# A: Distancia euclidea #
#########################
d = dist(data)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos k=2 clusters claros
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # El mayor silhouette se obtiene con k=2
#comparemos las particiones obtenidas para PAM y hclust
k <- 2
pam <- pam(d,2)
particiones <- comparar_particiones(k, hc, pam)
# Vemos que son casi iguales
# Hay un solo elemento que no sale en el mismo cluster
# veamos cual es
particionWard <- particiones[[1]]
particionPam <- particiones[[2]]
rbind(particionWard[particionPam==1]) # todos los ward y los que PAM sean 1
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.66 de media
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
library(cluster)
library(ggplot2)
#setwd("C:/Users/Alex/Desktop/Entrega3")
setwd("C:/UPV/Master/Analisis de datos/Entrega/Entrega3")
data = read.csv("indicedelitos.csv", header=TRUE, sep=" ")
#########################
# FUNCIONES COMUNES #
#########################
?hclust
calcular_dendrograma <- function(d, method="ward.D2"){
hc <- hclust(d, method= method)
plot(hc, hang=-0.1)
return (hc)
}
calcular_silhouettes <- function (d, n=10){
silPam = rep(NA, n)
for(k in 2:n)
{
#pam
p = pam(d, k, nstart=10)$clustering # en PAM hay que pasar la distancia
aux = silhouette(p, d) # aux es una matriz, lo que nosotros buscamos está en la tercera columna
silPam[k] = mean(aux[, 3])#sil_width
}
plot(1:n, silPam, type="b",xlab="Número de clusters", ylab="Silhouette width",ylim = c(0,1))
}
comparar_particiones <- function(k, hcObject, pamObject){
clward = cutree(hcObject, k)
clpam = pamObject$clustering
print(table(clward, clpam))
return(list(clward,clpam))
}
interpretar_clusters <- function(clpam, data){
for(cluster in 1:2)
{
selec = clpam==cluster
aux = apply(data[selec,], 2, function(x){c(mean(x), sd(x))})
cat("\nCluster ", cluster, "\n")
print(aux)
}
}
calcular_scree_graph <- function(d, k_p){
# Utilizando las coordenadas principales...
out = cmdscale(as.dist(d), k=k_p, eig=T, x.ret=T)
# Veamos los valores propios
plot(out$eig, type="b", xlim=c(0,10))
return(out)
}
#########################
# A: Distancia euclidea #
#########################
d = dist(data)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos k=2 clusters claros
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # El mayor silhouette se obtiene con k=2
#comparemos las particiones obtenidas para PAM y hclust
k <- 2
pam <- pam(d,2)
particiones <- comparar_particiones(k, hc, pam)
# Vemos que son casi iguales
# Hay un solo elemento que no sale en el mismo cluster
# veamos cual es
particionWard <- particiones[[1]]
particionPam <- particiones[[2]]
rbind(particionWard[particionPam==1]) # todos los ward y los que PAM sean 1
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.66 de media
plot(silPam, main = 'Plot silhouette: Partición PAM y distancias Euclídeas') #0.66 de media
silWard = silhouette(particionWard, d)
plot(silWard, main = 'Plot silhouette: Partición Ward.D2 y distancias euclídeas')#0.65
#teniendo en cuenta la particion que nos ha hecho el metodo pam
#interpretamos los datos: Calculamos la media de cada variable en cada cluster
interpretar_clusters(particionPam, data)
#aplicamos cmdscale
out <- calcular_scree_graph(d, 2)
# La mayor parte de la información está representada con el primer valor
# Información que se retiene en cada caso
q = 1
sum(out$eig[1:q]) / sum(out$eig) # 0.98
q = 2
sum(out$eig[1:q]) / sum(out$eig) # 0.998
# Creamos los dataframe para el grafico
df1 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionPam), Metodo = "PAM")
df2 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionWard), Metodo = "Ward")
df_combined <- rbind(df1, df2)
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = df_combined[51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
geom_point(data = df_combined[57+51, ], aes(C1, C2), shape = 21, color = "black", size = 3, stroke = 1.5) +
geom_text(data = df_combined[57+51, ], aes(C1, C2, label = "Ciudad 51"), vjust = -1.5, hjust = 0.5, show.legend = FALSE) +
facet_wrap(~Metodo)
g
############################
# B: distancia correlación #
############################
c = cor(t(data), t(data)) #ponemos individuos en columnas
d = sqrt(1-c)
d = as.dist(d)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos que pueden haber 2 o 3 clusters
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # Lo mejor es k=10, pero no es viable
#comparemos las particiones obtenidas en ambos casos
k <- 2
pam <- pam(d,k)
particiones <- comparar_particiones(k, hc, pam)
# En este caso hay 7 individuos clasificados de diferente manera
particionWard <- particiones[[1]]
particionPam <- particiones[[2]]
rbind(particionWard[particionPam==1]) # todos los ward y los que PAM sean 1
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.61 de media, hay algunos individuos mal clasificados
silWard = silhouette(particionWard, d)
plot(silWard)#0.61 tambien, pero el cluster 2 es muy malo
#teniendo en cuenta la particion que nos ha hecho el metodo pam
#interpretamos los datos: Calculamos la media de cada variable en cada cluster
interpretar_clusters(particionPam, data)
############################
# Reducción de las dimensiones
#aplicamos cmdscale
out <- calcular_scree_graph(d, 2)
# La mayor parte de la información está representada con el primer valor
# Información que se retiene en cada caso
q = 2
sum(out$eig[1:q])# mantenemos solo un 77% de variabilidad
# Creamos los dataframe para el grafico
df1 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionPam), Metodo = "PAM")
df2 <- data.frame(C1 = out$points[,1], C2 = out$points[,2], Cluster = factor(particionWard), Metodo = "Ward")
df_combined <- rbind(df1, df2)
g <- ggplot(df_combined, aes(C1, C2, color = Cluster)) +
geom_point(size = 3) +
facet_wrap(~Metodo)
g
############################
# B: distancia correlación #
############################
c = cor(t(data), t(data)) #ponemos individuos en columnas
d = sqrt(1-c)
d = as.dist(d)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos que pueden haber 2 o 3 clusters
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # Lo mejor es k=10, pero no es viable
calcular_silhouettes <- function (d, n=10){
silPam = rep(NA, n)
for(k in 2:n)
{
#pam
p = pam(d, k, nstart=10)$clustering # en PAM hay que pasar la distancia
aux = silhouette(p, d) # aux es una matriz, lo que nosotros buscamos está en la tercera columna
silPam[k] = mean(aux[, 3])#sil_width
}
plot(1:n, silPam, type="b",main="Índice de silhouette para distintas particiones PAM",xlab="Número de clusters", ylab="Silhouette width")
}
############################
# B: distancia correlación #
############################
c = cor(t(data), t(data)) #ponemos individuos en columnas
d = sqrt(1-c)
d = as.dist(d)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos que pueden haber 2 o 3 clusters
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # Lo mejor es k=10, pero no es viable
############################
# B: distancia correlación #
############################
c = cor(t(data)) #ponemos individuos en columnas
d = sqrt(1-c)
d = as.dist(d)
############################
# B: distancia correlación #
############################
c = cor(t(data)) #ponemos individuos en columnas
d = sqrt(1-c)
d = as.dist(d)
# ¿Cuántos clusters crear?
# metodo jerarquico
hc <- calcular_dendrograma(d) # Analizando el gráfico vemos que pueden haber 2 o 3 clusters
# Metodo particional para comprobarlo
calcular_silhouettes(d,15) # Lo mejor es k=10, pero no es viable
#comparemos las particiones obtenidas en ambos casos
k <- 3
pam <- pam(d,k)
particiones <- comparar_particiones(k, hc, pam)
# En este caso hay 7 individuos clasificados de diferente manera
particionWard <- particiones[[1]]
particionPam <- particiones[[2]]
rbind(particionWard[particionPam==1]) # todos los ward y los que PAM sean 1
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.61 de media, hay algunos individuos mal clasificados
silWard = silhouette(particionWard, d)
plot(silWard)#0.61 tambien, pero el cluster 2 es muy malo
#cual es mejor?
#sabemos que entre todos los valores de k posibles para pam, k=2 es el mejor
#pero es mejor pam en 2 cluster, o ward en 2 cluster?
silPam = silhouette(particionPam, d)
plot(silPam) #0.61 de media, hay algunos individuos mal clasificados
plot(silPam,main = "Plot silhouette: Partición PAM y distancia de correlaciones") #0.61 de media, hay algunos individuos mal clasificados
plot(silWard, main = "Plot silhouette: Partición Ward.D2 y distancia de correlaciones")#0.61 tambien, pero el cluster 2 es muy malo
